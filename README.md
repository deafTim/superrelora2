# SuperReLoRA

SuperReLoRA is an efficient fine-tuning method for large language models that combines the benefits of LoRA (Low-Rank Adaptation) with partial weight merging. This implementation provides a flexible and memory-efficient way to fine-tune transformer models.

## Features

- Partial weight merging during training
- Support for HuggingFace models and datasets
- Configurable LoRA parameters (rank, alpha, merge frequency)
- Both manual training loop and HuggingFace Trainer support
- Mixed precision training support
- Easy-to-use configuration system

## Installation

1. Clone the repository:
```bash
git clone https://github.com/deafTim/superrelora2.git
cd superrelora
```

2. Create a virtual environment and install dependencies:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

## Usage

### Training

1. Configure your training parameters in `training_configs/superrelora_160m.yaml`

2. Run training using the provided script:
```bash
# Using HuggingFace Trainer
python scripts/train_superrelora.py --config training_configs/superrelora_160m.yaml --use_trainer

# Using manual training loop
python scripts/train_superrelora.py --config training_configs/superrelora_160m.yaml
```

### Running on A100

For running on an A100 GPU, use the provided script:
```bash
bash scripts/run_on_a100.sh
```

## Configuration

The training configuration is specified in YAML format. Key parameters include:

- `model_name`: Base model to use
- `dataset_name`: Dataset for training
- `lora_r`: Rank of LoRA decomposition
- `lora_alpha`: Scaling factor
- `merge_every`: Steps between partial merges
- Training parameters (batch size, learning rate, etc.)

## Project Structure

```
superrelora/
â”‚
â”œâ”€â”€ src/                       # Core implementation
â”‚   â”œâ”€â”€ superrelora_linear.py  # SuperReLoRALinear class
â”‚   â”œâ”€â”€ superrelora_model.py   # Model wrapper
â”‚   â””â”€â”€ utils.py              # Utility functions
â”‚
â”œâ”€â”€ scripts/                   # Training scripts
â”‚   â”œâ”€â”€ train_superrelora.py  # Training script
â”‚   â””â”€â”€ run_on_a100.sh        # A100 setup script
â”‚
â”œâ”€â”€ configs/                   # Model configurations
â”‚   â””â”€â”€ llama_160m.json       # Tiny LLaMA config
â”‚
â”œâ”€â”€ training_configs/          # Training configurations
â”‚   â””â”€â”€ superrelora_160m.yaml # Training parameters
â”‚
â”œâ”€â”€ notebooks/                 # Analysis notebooks
â”‚   â”œâ”€â”€ check_ranks.ipynb
â”‚   â””â”€â”€ plot_loss.ipynb
â”‚
â”œâ”€â”€ results/                   # Training outputs
â”‚   â””â”€â”€ loss_curve.png
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details. 


## ðŸ“‰ Training Loss (TensorBoard)

![Training Loss](TensorBoardLoss.png)


## ðŸ“Š Evaluation Results

| Model             | Prompts | Accuracy | Perplexity | Final Loss |
|------------------|---------|----------|------------|-------------|
| Base (160M)       | 1000    | 0.0324   | 10.39      | 2.34        |
| SuperReLoRA (160M)| 1000    | 0.0412   | 8.95       | 2.10        |

*Quick evaluation on Wikitext-2-raw-v1, max_length=64, batch_size=16*
