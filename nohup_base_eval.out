/home/jupyter/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at nicholasKluge/TeenyTinyLlama-160m and are newly initialized: ['model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/jupyter/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Using device: cuda

Loading base model and tokenizer...
Model loaded successfully!

Loading dataset...
Dataset loaded with 1000 examples

Tokenizing dataset...
Tokenization complete!
Created dataloader with 63 batches

Starting metrics computation...

Starting evaluation on 63 batches...
Computing metrics:   0%|          | 0/63 [00:00<?, ?it/s]Computing metrics:   2%|▏         | 1/63 [00:02<02:11,  2.13s/it]Computing metrics:   8%|▊         | 5/63 [00:02<00:19,  2.95it/s]Computing metrics:  14%|█▍        | 9/63 [00:02<00:09,  5.98it/s]Computing metrics:  22%|██▏       | 14/63 [00:02<00:04, 10.37it/s]Computing metrics:  30%|███       | 19/63 [00:02<00:02, 15.04it/s]Computing metrics:  38%|███▊      | 24/63 [00:02<00:01, 19.72it/s]Computing metrics:  46%|████▌     | 29/63 [00:02<00:01, 24.11it/s]Computing metrics:  54%|█████▍    | 34/63 [00:02<00:01, 28.01it/s]Computing metrics:  62%|██████▏   | 39/63 [00:03<00:00, 31.29it/s]Computing metrics:  70%|██████▉   | 44/63 [00:03<00:00, 33.94it/s]Computing metrics:  78%|███████▊  | 49/63 [00:03<00:00, 36.01it/s]Computing metrics:  86%|████████▌ | 54/63 [00:03<00:00, 37.58it/s]Computing metrics:  94%|█████████▎| 59/63 [00:03<00:00, 38.73it/s]Computing metrics: 100%|██████████| 63/63 [00:03<00:00, 17.37it/s]
Both `max_new_tokens` (=1024) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
2025-06-15 23:17:18.201712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Both `max_new_tokens` (=1024) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)

Processing batch 1/63
Batch shape: torch.Size([64, 16])
Batch 1 - Loss: 13.9887, Accuracy: 0.0396

Processing batch 2/63
Batch shape: torch.Size([64, 16])
Batch 2 - Loss: 13.7634, Accuracy: 0.0269

Processing batch 3/63
Batch shape: torch.Size([64, 16])
Batch 3 - Loss: 14.8297, Accuracy: 0.0318

Processing batch 4/63
Batch shape: torch.Size([64, 16])
Batch 4 - Loss: 13.8601, Accuracy: 0.0250

Processing batch 5/63
Batch shape: torch.Size([64, 16])
Batch 5 - Loss: 14.9590, Accuracy: 0.0302

Processing batch 6/63
Batch shape: torch.Size([64, 16])
Batch 6 - Loss: 17.2007, Accuracy: 0.0571

Processing batch 7/63
Batch shape: torch.Size([64, 16])
Batch 7 - Loss: 17.1604, Accuracy: 0.0401

Processing batch 8/63
Batch shape: torch.Size([64, 16])
Batch 8 - Loss: 14.9322, Accuracy: 0.0328

Processing batch 9/63
Batch shape: torch.Size([64, 16])
Batch 9 - Loss: 14.4970, Accuracy: 0.0261

Processing batch 10/63
Batch shape: torch.Size([64, 16])
Batch 10 - Loss: 11.8713, Accuracy: 0.0243

Processing batch 11/63
Batch shape: torch.Size([64, 16])
Batch 11 - Loss: 14.0353, Accuracy: 0.0390

Processing batch 12/63
Batch shape: torch.Size([64, 16])
Batch 12 - Loss: 12.8348, Accuracy: 0.0200

Processing batch 13/63
Batch shape: torch.Size([64, 16])
Batch 13 - Loss: 15.1739, Accuracy: 0.0350

Processing batch 14/63
Batch shape: torch.Size([64, 16])
Batch 14 - Loss: 13.7559, Accuracy: 0.0234

Processing batch 15/63
Batch shape: torch.Size([64, 16])
Batch 15 - Loss: 13.8526, Accuracy: 0.0250

Processing batch 16/63
Batch shape: torch.Size([64, 16])
Batch 16 - Loss: 15.4174, Accuracy: 0.0442

Processing batch 17/63
Batch shape: torch.Size([64, 16])
Batch 17 - Loss: 14.8116, Accuracy: 0.0359

Processing batch 18/63
Batch shape: torch.Size([64, 16])
Batch 18 - Loss: 16.0658, Accuracy: 0.0307

Processing batch 19/63
Batch shape: torch.Size([64, 16])
Batch 19 - Loss: 15.1138, Accuracy: 0.0394

Processing batch 20/63
Batch shape: torch.Size([64, 16])
Batch 20 - Loss: 14.4039, Accuracy: 0.0262

Processing batch 21/63
Batch shape: torch.Size([64, 16])
Batch 21 - Loss: 16.0187, Accuracy: 0.0427

Processing batch 22/63
Batch shape: torch.Size([64, 16])
Batch 22 - Loss: 13.8272, Accuracy: 0.0356

Processing batch 23/63
Batch shape: torch.Size([64, 16])
Batch 23 - Loss: 14.2091, Accuracy: 0.0249

Processing batch 24/63
Batch shape: torch.Size([64, 16])
Batch 24 - Loss: 15.5489, Accuracy: 0.0312

Processing batch 25/63
Batch shape: torch.Size([64, 16])
Batch 25 - Loss: 14.6717, Accuracy: 0.0334

Processing batch 26/63
Batch shape: torch.Size([64, 16])
Batch 26 - Loss: 16.0517, Accuracy: 0.0420

Processing batch 27/63
Batch shape: torch.Size([64, 16])
Batch 27 - Loss: 16.4277, Accuracy: 0.0400

Processing batch 28/63
Batch shape: torch.Size([64, 16])
Batch 28 - Loss: 13.3070, Accuracy: 0.0281

Processing batch 29/63
Batch shape: torch.Size([64, 16])
Batch 29 - Loss: 15.3740, Accuracy: 0.0260

Processing batch 30/63
Batch shape: torch.Size([64, 16])
Batch 30 - Loss: 15.0455, Accuracy: 0.0265

Processing batch 31/63
Batch shape: torch.Size([64, 16])
Batch 31 - Loss: 17.6929, Accuracy: 0.0729

Processing batch 32/63
Batch shape: torch.Size([64, 16])
Batch 32 - Loss: 17.0579, Accuracy: 0.0556

Processing batch 33/63
Batch shape: torch.Size([64, 16])
Batch 33 - Loss: 15.5450, Accuracy: 0.0878

Processing batch 34/63
Batch shape: torch.Size([64, 16])
Batch 34 - Loss: 15.4553, Accuracy: 0.0878

Processing batch 35/63
Batch shape: torch.Size([64, 16])
Batch 35 - Loss: 16.1611, Accuracy: 0.0447

Processing batch 36/63
Batch shape: torch.Size([64, 16])
Batch 36 - Loss: 14.4701, Accuracy: 0.0259

Processing batch 37/63
Batch shape: torch.Size([64, 16])
Batch 37 - Loss: 14.2279, Accuracy: 0.0305

Processing batch 38/63
Batch shape: torch.Size([64, 16])
Batch 38 - Loss: 15.3144, Accuracy: 0.0295

Processing batch 39/63
Batch shape: torch.Size([64, 16])
Batch 39 - Loss: 14.8512, Accuracy: 0.0295

Processing batch 40/63
Batch shape: torch.Size([64, 16])
Batch 40 - Loss: 15.3304, Accuracy: 0.0328

Processing batch 41/63
Batch shape: torch.Size([64, 16])
Batch 41 - Loss: 10.9213, Accuracy: 0.1288

Processing batch 42/63
Batch shape: torch.Size([64, 16])
Batch 42 - Loss: 14.2984, Accuracy: 0.0295

Processing batch 43/63
Batch shape: torch.Size([64, 16])
Batch 43 - Loss: 12.8372, Accuracy: 0.0323

Processing batch 44/63
Batch shape: torch.Size([64, 16])
Batch 44 - Loss: 16.2273, Accuracy: 0.0395

Processing batch 45/63
Batch shape: torch.Size([64, 16])
Batch 45 - Loss: 15.4074, Accuracy: 0.0329

Processing batch 46/63
Batch shape: torch.Size([64, 16])
Batch 46 - Loss: 12.8341, Accuracy: 0.0232

Processing batch 47/63
Batch shape: torch.Size([64, 16])
Batch 47 - Loss: 16.7723, Accuracy: 0.0326

Processing batch 48/63
Batch shape: torch.Size([64, 16])
Batch 48 - Loss: 15.0451, Accuracy: 0.0301

Processing batch 49/63
Batch shape: torch.Size([64, 16])
Batch 49 - Loss: 14.4862, Accuracy: 0.0355

Processing batch 50/63
Batch shape: torch.Size([64, 16])
Batch 50 - Loss: 14.8384, Accuracy: 0.0475

Processing batch 51/63
Batch shape: torch.Size([64, 16])
Batch 51 - Loss: 15.0086, Accuracy: 0.0286

Processing batch 52/63
Batch shape: torch.Size([64, 16])
Batch 52 - Loss: 16.1630, Accuracy: 0.0360

Processing batch 53/63
Batch shape: torch.Size([64, 16])
Batch 53 - Loss: 15.5980, Accuracy: 0.0356

Processing batch 54/63
Batch shape: torch.Size([64, 16])
Batch 54 - Loss: 14.3327, Accuracy: 0.0351

Processing batch 55/63
Batch shape: torch.Size([64, 16])
Batch 55 - Loss: 15.4770, Accuracy: 0.0280

Processing batch 56/63
Batch shape: torch.Size([64, 16])
Batch 56 - Loss: 16.5483, Accuracy: 0.0467

Processing batch 57/63
Batch shape: torch.Size([64, 16])
Batch 57 - Loss: 16.1672, Accuracy: 0.0292

Processing batch 58/63
Batch shape: torch.Size([64, 16])
Batch 58 - Loss: 16.4124, Accuracy: 0.0730

Processing batch 59/63
Batch shape: torch.Size([64, 16])
Batch 59 - Loss: 11.6785, Accuracy: 0.1603

Processing batch 60/63
Batch shape: torch.Size([64, 16])
Batch 60 - Loss: 16.8669, Accuracy: 0.0353

Processing batch 61/63
Batch shape: torch.Size([64, 16])
Batch 61 - Loss: 14.2373, Accuracy: 0.0312

Processing batch 62/63
Batch shape: torch.Size([64, 16])
Batch 62 - Loss: 14.9134, Accuracy: 0.0213

Processing batch 63/63
Batch shape: torch.Size([64, 8])
Batch 63 - Loss: 13.0926, Accuracy: 0.0243

Evaluation complete!
Total tokens processed: 26542
Total correct predictions: 934

Base Model Metrics (Quick Evaluation):
Loss: 2.2649
Perplexity: 9.63
Accuracy: 0.0352

Generating example texts:

Prompt: Once upon a time
Generated: Once upon a time in each one I got to fight with you? I am wonderful lady from the same people who make been surely found these are as much. Please now and because it's not really difficult, I wish to have any huge managers and dream-heats. You can do this entry for some of all hearts, and also will come back on your love and take me out of a lot.
We’ll still have about us whom we said that I know you are so happy. We don’t never spend itself by it
I’m very well reaffirmed anything!
Bem-vindo ao blog da Berenice, hoje reunimos aqui para te dar um abraço e até dia 15 de outubro.
O Blog é uma organização do grupo DHB (www.dhb.org.br), com o intuito de divulgar os trabalhos realizados por nossos colaboradores através dos mais diversos meios.
Para deixarmos registrado nosso agradecimento à equipe pela dedicação que dedicamos aos seus trabalhos, esperamos contar com toda a sua experiencia neste momento tão importante na vida de qualquer pessoa.
You will visit my blog across your face: http://www.youtube.com/user/simposted_blog/
Fique atento(a) ao restante desta postagem.
Use a imaginação e faça a diferença!
Palavras-chave: #conhecimento, #educação #feminino #trabalho, #escolha, #amor, #amor
Tags: #conhecimento, #empreendedorismo, #marketing
← Post anterior | Mais um trabalho realizado por nós
Avançar Próximo post →
Deixe um comentário Cancelar resposta
O seu endereço de e-mail não será publicado. Campos obrigatórios são marcados com *
Comentário
Nome*
E-mail*
Website
Salvar meus dados neste navegador para a próxima vez que eu comentar.
Artigos Relacionados
Maria Anastácia – 30 anos – Sucesso
Candidatos da lista do PPL terão novas contratações em janeiro
Mariela Schmitz – 3 meses atrás
Categorias
Artigos
Tendências
Hipnose Clínica e Personal Releases
Como fazer um autoconhecimento?
Vale Tudo
Blog
Contato
Grupo DHB - Todos os direitos reservados.
DHB Brasil
Rua Tio Sam, 126 – Centro – São Paulo/SP (31) 3322-3000
© 2021 - Grupo DHB. Todos os Direitos Reservados. Designed by GDHG.com.
Política de Privacidade
Este site utiliza cookies para melhorar sua experiência de navegação. Se você continuar a navegar, consideraremos que você aceita nossa Política de Cookies. Se você já estiver satisfeito com a utilização de cookies, você pode optar pelo uso de alguns de nossos recursos e também de acordo com nossas Políticas de privacidade.
Configurações
Close
Privacy Overview
This website uses cookies to improve your experience while you navigate through the website. Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may affect your browsing experience.
Necessário
Necessário
Sempre ativado
Os cookies necessários são absolutamente essenciais para o funcionamento adequado do site. Esses cookies garantem funcionalidades básicas e recursos de segurança do site, de forma anônima.
Cookie
Duração
Descrição
cookielawinfo-checbox-analytics 11 months Este cookie é definido pelo plug-in GDPR Cookie Consent. É usado para armazenar o consentimento do usuário para visitar os sites 24 horas por dia. O cookie armazena informações sobre como os visitantes usam este site, incluindo o número de ID de utilizador, a data de expiração e o seu geolocator. O cookie apenas usa o nome do cookie genérico armazenado no seu navegador, sem a informação pessoal necessária para o endereço do usuário. Os ID's só podem ser definidos em função da categoria do cookie sendo usada.
Funcionais
funcionais
Os cookies funcionais ajudam a realizar certas funcionalidades, como compartilhar o conteúdo do site nas redes sociais, coletar feedbacks e outros recursos de terceiros.
Desempenho
Desempenho
Cookies analíticos são usados ​​para entender como os visitantes interagem com o site. Esses cookies ajudam a fornecer informações sobre métricas de número de visitantes, taxa de rejeição, origem de tráfego, etc.
Anúncio
Anunciante
Anunciado por: Google Analytics. O Google Analytics coleta informações anônimas, como o número de visitantes do site, o tempo gasto pelos usuários durante a interação com
--------------------------------------------------

Prompt: The most important thing about
Generated: The most important thing about it, but I will come to the world that have all itself as a good party soon and if I can get more than 100 people follow me. It's not differently better for she didn't be like in love, but you don’t begin to do it! All millions of travelers is worst. You are great ideas, each one will trick on your blogger away from my cloud? Before you look, you see the orange-black point and now this week are always around here! That’s my gift, which is straight together with your name or 
Continue reading
Bookmark
Related posts:
2 Comentários
You May Also Like
Show de bola
27 novembro, 2012
Copyright © 2021 - Bookmark / Compete®
Search for:
Searchable
Posts Recentes
Promover Bibliografias e Artigos em Revista e Pesquisa
Revista Biblioteca Digital Brasileira
Data de Publicação
24/11/2014
0
O que é uma Obra Literária?
21/05/2015
0
A Escrita Literária, como método de estudo da literatura
31/03/2017
0
Histórias que não Ficam Perdidas – Contos de Fada – Editora Ática
28/10/2019
0
Viajar Faz Pouco Tempo – O Segredo das Crianças
11/06/2018
0
Nacionalistas de Língua Portuguesa – Práxis de Literatura
10/08/2016
0
Literatura por Mim – A História da Literatura
14/01/2005
0
Maiores Livros – Série Dramáticas
16/09/2010
0
Gerações do Brasil – Histórias das Gerações
15/02/2009
0
O que eu sei sobre o livro “Escritos” – Crônicas de Viagem
19/04/2012
0
Inspiração no Diabo – A Arte da Encantar
18/mar/2012
0
Tags Populares
Folkart
Arquivos
Arquivos Selecionados (1) agosto 2013 (2) julho 2013 (3) abril 2013 (4) março 2013 (5) fevereiro 2013 (7) janeiro 2013 (5) dezembro 2012 (11) novembro 2012 (14) outubro 2012 (22) setembro 2012 (6) agosto 2012 (10) julho 2012 (26) junho 2012 (30) maio 2012 (24) abril 2012 (19) março 2012 (21) fevereiro 2012 (22) janeiro 2012 (27) dezembro 2011 (28) novembro 2011 (29) outubro 2011 (31) setembro 2011 (43) agosto 2011 (58) julho 2011 (52) junho 2011 (6)
Categorias
Artigo (1)
Artigos (7)
Publicações (1.129)
Sobre o Autor (91)
Geral (1.363)
Mídia (1.196)
Notícias (1.208)
Outros (54)
Tecnologia (218)
Tema Livre (104)
Uncategorized (25)
Série DRAMA (5)
Sexo & Feminismo (1)
Dicas e Truques (205)
Concursos (915)
Eventos (513)
Filmes (411)
Humor (925)
Livros (446)
Diário de um escritor adulto (257)
Filme (283)
Fotografia (102)
Livro (148)
Meu nome não é ninguém (10)
Novas histórias (149)
Outras Linhas (1899)
The Tale of a Joker (1822)
The Love Boy: A Life in History (1847)
Algo que nunca pensei (186)
Contato (176)
Arquivo Seleccionado com #PlatinaBR
Escrito nas últimas décadas pela autora Anne Frank (Editora Arqueiro, 2014). Com uma narrativa envolvente e divertida, este conto mostra como cada personagem adquire características próprias. Para alguns leitores este é apenas um exemplo da capacidade de se tornar uma escritora independente quando comparada a outros escritores. De forma geral, trata-se de uma história bem escrita. No dia 17 de Outubro de 2020, o Governo do Estado do Ceará recebeu a visita dos técnicos administrativos do Instituto Brasília Ambiental, do Ministério do Meio Ambiente (MMA), da Secretaria Estadual de Desenvolvimento Sustentável (SEDESA) e da Fundação Nacional do Índio (FUNAI) para reunião de trabalho conjunta envolvendo secretarias estaduais.
“Estamos dando continuidade às ações e trabalhos conjuntos realizados pelo Ibama com os órgãos ambientais estaduais, buscando alternativas para fortalecer a cadeia produtiva do agronegócio estadual”, afirmou o secretário estadual de Agricultura, Pecuária e Abastecimento, Edson Leal.
De acordo com o coordenador técnico do IBAMA, Flávio José Fernandes, além da parceria entre a FUNAI e o Ibama, o governador Camilo Santana dará mais destaque ao tema do desenvolvimento sustentável na região.
“O objetivo da reunião foi o de aproximar os setores produtivos, fortalecendo a necessidade de ações integradas entre os atores envolvidos na cadeia produtiva do agro. Além disso, também foram discutidas estratégias conjuntas voltadas à diminuição do impacto ambiental gerado pelas atividades agropecuárias”, disse.
A secretária municipal de Desenvolvimento Territorial e Sustentabilidade (Sedese), Caroline Bezerra, destacou que esta é mais uma oportunidade de trazer informações relevantes para os agricultores familiares de áreas urbanas e rurais de todo o estado. “É necessário reforçar a importância do setor ser produtor rural
--------------------------------------------------
