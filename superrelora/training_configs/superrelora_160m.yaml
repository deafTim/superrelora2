# Model configuration
model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # Base model to use
dataset_name: "wikitext"  # Dataset to use for training

# SuperReLoRA specific parameters
lora_r: 8  # Rank of LoRA decomposition
lora_alpha: 16.0  # Scaling factor
merge_every: 100  # Steps between partial merges

# Training parameters
num_epochs: 3
batch_size: 8
learning_rate: 2e-4
weight_decay: 0.01
max_length: 512  # Maximum sequence length

# Optimizer parameters
optimizer:
  type: "adamw"
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8

# Learning rate scheduler
scheduler:
  type: "cosine"
  warmup_steps: 100
  total_steps: 1000

# Logging and checkpointing
logging_steps: 100
save_steps: 1000
eval_steps: 1000

# Mixed precision training
fp16: true
bf16: false

# Gradient accumulation
gradient_accumulation_steps: 4

# Target modules for LoRA
target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj" 