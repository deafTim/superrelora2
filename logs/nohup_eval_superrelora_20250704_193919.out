/home/jupyter/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at nicholasKluge/TeenyTinyLlama-160m and are newly initialized: ['model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/jupyter/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading model and tokenizer...
Checkpoint keys: ['model.model.embed_tokens.weight', 'model.model.layers.0.self_attn.q_proj.weight', 'model.model.layers.0.self_attn.q_proj.lora_A.weight', 'model.model.layers.0.self_attn.q_proj.lora_B.weight', 'model.model.layers.0.self_attn.k_proj.weight', 'model.model.layers.0.self_attn.k_proj.lora_A.weight', 'model.model.layers.0.self_attn.k_proj.lora_B.weight', 'model.model.layers.0.self_attn.v_proj.weight', 'model.model.layers.0.self_attn.v_proj.lora_A.weight', 'model.model.layers.0.self_attn.v_proj.lora_B.weight']
Model state_dict keys: ['model.model.embed_tokens.weight', 'model.model.layers.0.self_attn.q_proj.weight', 'model.model.layers.0.self_attn.q_proj.lora_A.weight', 'model.model.layers.0.self_attn.q_proj.lora_B.weight', 'model.model.layers.0.self_attn.k_proj.weight', 'model.model.layers.0.self_attn.k_proj.lora_A.weight', 'model.model.layers.0.self_attn.k_proj.lora_B.weight', 'model.model.layers.0.self_attn.v_proj.weight', 'model.model.layers.0.self_attn.v_proj.lora_A.weight', 'model.model.layers.0.self_attn.v_proj.lora_B.weight']
Loading dataset...
Computing metrics...

Starting evaluation on 125 batches...
Computing metrics:   0%|          | 0/125 [00:00<?, ?it/s]Computing metrics:   1%|          | 1/125 [00:00<01:03,  1.96it/s]Computing metrics:   2%|▏         | 2/125 [00:00<00:34,  3.54it/s]Computing metrics:   2%|▏         | 3/125 [00:00<00:25,  4.87it/s]Computing metrics:   3%|▎         | 4/125 [00:00<00:20,  6.00it/s]Computing metrics:   5%|▍         | 6/125 [00:01<00:15,  7.69it/s]Computing metrics:   6%|▋         | 8/125 [00:01<00:13,  8.66it/s]Computing metrics:   8%|▊         | 10/125 [00:01<00:12,  9.22it/s]Computing metrics:  10%|▉         | 12/125 [00:01<00:11,  9.61it/s]Computing metrics:  11%|█         | 14/125 [00:01<00:11,  9.80it/s]Computing metrics:  12%|█▏        | 15/125 [00:01<00:11,  9.83it/s]Computing metrics:  14%|█▎        | 17/125 [00:02<00:10,  9.98it/s]Computing metrics:  15%|█▌        | 19/125 [00:02<00:10, 10.09it/s]Computing metrics:  17%|█▋        | 21/125 [00:02<00:10, 10.11it/s]Computing metrics:  18%|█▊        | 23/125 [00:02<00:09, 10.25it/s]Computing metrics:  20%|██        | 25/125 [00:02<00:09, 10.22it/s]Computing metrics:  22%|██▏       | 27/125 [00:03<00:09, 10.11it/s]Computing metrics:  23%|██▎       | 29/125 [00:03<00:09, 10.11it/s]Computing metrics:  25%|██▍       | 31/125 [00:03<00:09, 10.17it/s]Computing metrics:  26%|██▋       | 33/125 [00:03<00:09, 10.21it/s]Computing metrics:  28%|██▊       | 35/125 [00:03<00:08, 10.24it/s]Computing metrics:  30%|██▉       | 37/125 [00:04<00:08, 10.32it/s]Computing metrics:  31%|███       | 39/125 [00:04<00:08, 10.46it/s]Computing metrics:  33%|███▎      | 41/125 [00:04<00:08, 10.39it/s]Computing metrics:  34%|███▍      | 43/125 [00:04<00:07, 10.51it/s]Computing metrics:  36%|███▌      | 45/125 [00:04<00:07, 10.45it/s]Computing metrics:  38%|███▊      | 47/125 [00:05<00:07, 10.37it/s]Computing metrics:  39%|███▉      | 49/125 [00:05<00:07, 10.28it/s]Computing metrics:  41%|████      | 51/125 [00:05<00:07, 10.20it/s]Computing metrics:  42%|████▏     | 53/125 [00:05<00:06, 10.33it/s]Computing metrics:  44%|████▍     | 55/125 [00:05<00:06, 10.28it/s]Computing metrics:  46%|████▌     | 57/125 [00:06<00:06, 10.28it/s]Computing metrics:  47%|████▋     | 59/125 [00:06<00:06, 10.28it/s]Computing metrics:  49%|████▉     | 61/125 [00:06<00:06, 10.18it/s]Computing metrics:  50%|█████     | 63/125 [00:06<00:06, 10.12it/s]Computing metrics:  52%|█████▏    | 65/125 [00:06<00:05, 10.13it/s]Computing metrics:  54%|█████▎    | 67/125 [00:06<00:05, 10.21it/s]Computing metrics:  55%|█████▌    | 69/125 [00:07<00:05, 10.18it/s]Computing metrics:  57%|█████▋    | 71/125 [00:07<00:05, 10.19it/s]Computing metrics:  58%|█████▊    | 73/125 [00:07<00:05, 10.21it/s]Computing metrics:  60%|██████    | 75/125 [00:07<00:04, 10.16it/s]Computing metrics:  62%|██████▏   | 77/125 [00:07<00:04, 10.11it/s]Computing metrics:  63%|██████▎   | 79/125 [00:08<00:04, 10.10it/s]Computing metrics:  65%|██████▍   | 81/125 [00:08<00:04, 10.26it/s]Computing metrics:  66%|██████▋   | 83/125 [00:08<00:03, 10.56it/s]Computing metrics:  68%|██████▊   | 85/125 [00:08<00:03, 10.50it/s]Computing metrics:  70%|██████▉   | 87/125 [00:08<00:03, 10.37it/s]Computing metrics:  71%|███████   | 89/125 [00:09<00:03, 10.24it/s]Computing metrics:  73%|███████▎  | 91/125 [00:09<00:03, 10.18it/s]Computing metrics:  74%|███████▍  | 93/125 [00:09<00:03, 10.21it/s]Computing metrics:  76%|███████▌  | 95/125 [00:09<00:02, 10.35it/s]Computing metrics:  78%|███████▊  | 97/125 [00:09<00:02, 10.35it/s]Computing metrics:  79%|███████▉  | 99/125 [00:10<00:02, 10.29it/s]Computing metrics:  81%|████████  | 101/125 [00:10<00:02, 10.33it/s]Computing metrics:  82%|████████▏ | 103/125 [00:10<00:02, 10.21it/s]Computing metrics:  84%|████████▍ | 105/125 [00:10<00:01, 10.11it/s]Computing metrics:  86%|████████▌ | 107/125 [00:10<00:01, 10.12it/s]Computing metrics:  87%|████████▋ | 109/125 [00:11<00:01, 10.18it/s]Computing metrics:  89%|████████▉ | 111/125 [00:11<00:01, 10.20it/s]Computing metrics:  90%|█████████ | 113/125 [00:11<00:01, 10.27it/s]Computing metrics:  92%|█████████▏| 115/125 [00:11<00:00, 10.38it/s]Computing metrics:  94%|█████████▎| 117/125 [00:11<00:00, 10.60it/s]Computing metrics:  95%|█████████▌| 119/125 [00:12<00:00, 10.71it/s]Computing metrics:  97%|█████████▋| 121/125 [00:12<00:00, 10.59it/s]Computing metrics:  98%|█████████▊| 123/125 [00:12<00:00, 10.42it/s]Computing metrics: 100%|██████████| 125/125 [00:12<00:00, 10.29it/s]Computing metrics: 100%|██████████| 125/125 [00:12<00:00,  9.91it/s]
Both `max_new_tokens` (=1024) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
2025-07-04 19:40:27.496568: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Both `max_new_tokens` (=1024) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Both `max_new_tokens` (=1024) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)

Processing batch 1/125
Batch shape: torch.Size([128, 8])
Batch 1 - Loss: 41.5045, Accuracy: 0.0179

Processing batch 2/125
Batch shape: torch.Size([128, 8])
Batch 2 - Loss: 26.0451, Accuracy: 0.0236

Processing batch 3/125
Batch shape: torch.Size([128, 8])
Batch 3 - Loss: 19.6204, Accuracy: 0.0134

Processing batch 4/125
Batch shape: torch.Size([128, 8])
Batch 4 - Loss: 30.1579, Accuracy: 0.0213

Processing batch 5/125
Batch shape: torch.Size([128, 8])
Batch 5 - Loss: 28.2504, Accuracy: 0.0160

Processing batch 6/125
Batch shape: torch.Size([128, 8])
Batch 6 - Loss: 29.3726, Accuracy: 0.0136

Processing batch 7/125
Batch shape: torch.Size([128, 8])
Batch 7 - Loss: 19.7305, Accuracy: 0.0171

Processing batch 8/125
Batch shape: torch.Size([128, 8])
Batch 8 - Loss: 33.7863, Accuracy: 0.0153

Processing batch 9/125
Batch shape: torch.Size([128, 8])
Batch 9 - Loss: 19.3075, Accuracy: 0.0168

Processing batch 10/125
Batch shape: torch.Size([128, 8])
Batch 10 - Loss: 45.7537, Accuracy: 0.0286

Processing batch 11/125
Batch shape: torch.Size([128, 8])
Batch 11 - Loss: 36.3220, Accuracy: 0.0180

Processing batch 12/125
Batch shape: torch.Size([128, 8])
Batch 12 - Loss: 35.0044, Accuracy: 0.0550

Processing batch 13/125
Batch shape: torch.Size([128, 8])
Batch 13 - Loss: 25.4298, Accuracy: 0.0258

Processing batch 14/125
Batch shape: torch.Size([128, 8])
Batch 14 - Loss: 21.3248, Accuracy: 0.0150

Processing batch 15/125
Batch shape: torch.Size([128, 8])
Batch 15 - Loss: 30.3279, Accuracy: 0.0150

Processing batch 16/125
Batch shape: torch.Size([128, 8])
Batch 16 - Loss: 28.9356, Accuracy: 0.0175

Processing batch 17/125
Batch shape: torch.Size([128, 8])
Batch 17 - Loss: 21.2631, Accuracy: 0.0208

Processing batch 18/125
Batch shape: torch.Size([128, 8])
Batch 18 - Loss: 19.3775, Accuracy: 0.0102

Processing batch 19/125
Batch shape: torch.Size([128, 8])
Batch 19 - Loss: 14.4848, Accuracy: 0.0156

Processing batch 20/125
Batch shape: torch.Size([128, 8])
Batch 20 - Loss: 19.1825, Accuracy: 0.0202

Processing batch 21/125
Batch shape: torch.Size([128, 8])
Batch 21 - Loss: 26.6424, Accuracy: 0.0167

Processing batch 22/125
Batch shape: torch.Size([128, 8])
Batch 22 - Loss: 18.5094, Accuracy: 0.0283

Processing batch 23/125
Batch shape: torch.Size([128, 8])
Batch 23 - Loss: 19.8205, Accuracy: 0.0105

Processing batch 24/125
Batch shape: torch.Size([128, 8])
Batch 24 - Loss: 19.6704, Accuracy: 0.0157

Processing batch 25/125
Batch shape: torch.Size([128, 8])
Batch 25 - Loss: 21.4022, Accuracy: 0.0151

Processing batch 26/125
Batch shape: torch.Size([128, 8])
Batch 26 - Loss: 20.4288, Accuracy: 0.0172

Processing batch 27/125
Batch shape: torch.Size([128, 8])
Batch 27 - Loss: 14.6325, Accuracy: 0.0156

Processing batch 28/125
Batch shape: torch.Size([128, 8])
Batch 28 - Loss: 41.3085, Accuracy: 0.0144

Processing batch 29/125
Batch shape: torch.Size([128, 8])
Batch 29 - Loss: 17.6579, Accuracy: 0.0169

Processing batch 30/125
Batch shape: torch.Size([128, 8])
Batch 30 - Loss: 19.3668, Accuracy: 0.0115

Processing batch 31/125
Batch shape: torch.Size([128, 8])
Batch 31 - Loss: 41.6710, Accuracy: 0.0180

Processing batch 32/125
Batch shape: torch.Size([128, 8])
Batch 32 - Loss: 51.2929, Accuracy: 0.0420

Processing batch 33/125
Batch shape: torch.Size([128, 8])
Batch 33 - Loss: 18.9353, Accuracy: 0.0312

Processing batch 34/125
Batch shape: torch.Size([128, 8])
Batch 34 - Loss: 37.2381, Accuracy: 0.0217

Processing batch 35/125
Batch shape: torch.Size([128, 8])
Batch 35 - Loss: 19.7070, Accuracy: 0.0192

Processing batch 36/125
Batch shape: torch.Size([128, 8])
Batch 36 - Loss: 51.8563, Accuracy: 0.0656

Processing batch 37/125
Batch shape: torch.Size([128, 8])
Batch 37 - Loss: 25.2691, Accuracy: 0.0496

Processing batch 38/125
Batch shape: torch.Size([128, 8])
Batch 38 - Loss: 20.8436, Accuracy: 0.0337

Processing batch 39/125
Batch shape: torch.Size([128, 8])
Batch 39 - Loss: 28.6698, Accuracy: 0.0309

Processing batch 40/125
Batch shape: torch.Size([128, 8])
Batch 40 - Loss: 25.4345, Accuracy: 0.0177

Processing batch 41/125
Batch shape: torch.Size([128, 8])
Batch 41 - Loss: 36.3858, Accuracy: 0.0293

Processing batch 42/125
Batch shape: torch.Size([128, 8])
Batch 42 - Loss: 22.8720, Accuracy: 0.0610

Processing batch 43/125
Batch shape: torch.Size([128, 8])
Batch 43 - Loss: 25.8036, Accuracy: 0.0256

Processing batch 44/125
Batch shape: torch.Size([128, 8])
Batch 44 - Loss: 27.2409, Accuracy: 0.0118

Processing batch 45/125
Batch shape: torch.Size([128, 8])
Batch 45 - Loss: 23.3880, Accuracy: 0.0175

Processing batch 46/125
Batch shape: torch.Size([128, 8])
Batch 46 - Loss: 23.0326, Accuracy: 0.0126

Processing batch 47/125
Batch shape: torch.Size([128, 8])
Batch 47 - Loss: 33.6623, Accuracy: 0.0138

Processing batch 48/125
Batch shape: torch.Size([128, 8])
Batch 48 - Loss: 41.5483, Accuracy: 0.0211

Processing batch 49/125
Batch shape: torch.Size([128, 8])
Batch 49 - Loss: 22.0399, Accuracy: 0.0278

Processing batch 50/125
Batch shape: torch.Size([128, 8])
Batch 50 - Loss: 51.1686, Accuracy: 0.0287

Processing batch 51/125
Batch shape: torch.Size([128, 8])
Batch 51 - Loss: 18.8925, Accuracy: 0.0133

Processing batch 52/125
Batch shape: torch.Size([128, 8])
Batch 52 - Loss: 27.1999, Accuracy: 0.0455

Processing batch 53/125
Batch shape: torch.Size([128, 8])
Batch 53 - Loss: 21.6233, Accuracy: 0.0417

Processing batch 54/125
Batch shape: torch.Size([128, 8])
Batch 54 - Loss: 42.7281, Accuracy: 0.0176

Processing batch 55/125
Batch shape: torch.Size([128, 8])
Batch 55 - Loss: 32.0079, Accuracy: 0.0194

Processing batch 56/125
Batch shape: torch.Size([128, 8])
Batch 56 - Loss: 20.3078, Accuracy: 0.0155

Processing batch 57/125
Batch shape: torch.Size([128, 8])
Batch 57 - Loss: 36.4153, Accuracy: 0.0179

Processing batch 58/125
Batch shape: torch.Size([128, 8])
Batch 58 - Loss: 21.2244, Accuracy: 0.0179

Processing batch 59/125
Batch shape: torch.Size([128, 8])
Batch 59 - Loss: 23.3585, Accuracy: 0.0273

Processing batch 60/125
Batch shape: torch.Size([128, 8])
Batch 60 - Loss: 19.3793, Accuracy: 0.0095

Processing batch 61/125
Batch shape: torch.Size([128, 8])
Batch 61 - Loss: 22.1920, Accuracy: 0.0323

Processing batch 62/125
Batch shape: torch.Size([128, 8])
Batch 62 - Loss: 52.4729, Accuracy: 0.0256

Processing batch 63/125
Batch shape: torch.Size([128, 8])
Batch 63 - Loss: 42.1956, Accuracy: 0.0387

Processing batch 64/125
Batch shape: torch.Size([128, 8])
Batch 64 - Loss: 43.5021, Accuracy: 0.0226

Processing batch 65/125
Batch shape: torch.Size([128, 8])
Batch 65 - Loss: 41.2879, Accuracy: 0.0174

Processing batch 66/125
Batch shape: torch.Size([128, 8])
Batch 66 - Loss: 41.2981, Accuracy: 0.0209

Processing batch 67/125
Batch shape: torch.Size([128, 8])
Batch 67 - Loss: 43.5612, Accuracy: 0.0153

Processing batch 68/125
Batch shape: torch.Size([128, 8])
Batch 68 - Loss: 40.9701, Accuracy: 0.0244

Processing batch 69/125
Batch shape: torch.Size([128, 8])
Batch 69 - Loss: 42.2815, Accuracy: 0.0201

Processing batch 70/125
Batch shape: torch.Size([128, 8])
Batch 70 - Loss: 52.6574, Accuracy: 0.0256

Processing batch 71/125
Batch shape: torch.Size([128, 8])
Batch 71 - Loss: 19.0955, Accuracy: 0.0152

Processing batch 72/125
Batch shape: torch.Size([128, 8])
Batch 72 - Loss: 42.1797, Accuracy: 0.0244

Processing batch 73/125
Batch shape: torch.Size([128, 8])
Batch 73 - Loss: 24.4994, Accuracy: 0.0145

Processing batch 74/125
Batch shape: torch.Size([128, 8])
Batch 74 - Loss: 19.9850, Accuracy: 0.0159

Processing batch 75/125
Batch shape: torch.Size([128, 8])
Batch 75 - Loss: 18.9778, Accuracy: 0.0210

Processing batch 76/125
Batch shape: torch.Size([128, 8])
Batch 76 - Loss: 27.5912, Accuracy: 0.0100

Processing batch 77/125
Batch shape: torch.Size([128, 8])
Batch 77 - Loss: 22.4428, Accuracy: 0.0201

Processing batch 78/125
Batch shape: torch.Size([128, 8])
Batch 78 - Loss: 35.2283, Accuracy: 0.0152

Processing batch 79/125
Batch shape: torch.Size([128, 8])
Batch 79 - Loss: 22.4742, Accuracy: 0.0177

Processing batch 80/125
Batch shape: torch.Size([128, 8])
Batch 80 - Loss: 22.1282, Accuracy: 0.0209

Processing batch 81/125
Batch shape: torch.Size([128, 8])
Batch 81 - Loss: 12.0736, Accuracy: 0.1047

Processing batch 82/125
Batch shape: torch.Size([128, 8])
Batch 82 - Loss: 12.6004, Accuracy: 0.1014

Processing batch 83/125
Batch shape: torch.Size([128, 8])
Batch 83 - Loss: 12.8323, Accuracy: 0.0800

Processing batch 84/125
Batch shape: torch.Size([128, 8])
Batch 84 - Loss: 27.9094, Accuracy: 0.0149

Processing batch 85/125
Batch shape: torch.Size([128, 8])
Batch 85 - Loss: 35.1039, Accuracy: 0.0276

Processing batch 86/125
Batch shape: torch.Size([128, 8])
Batch 86 - Loss: 23.2821, Accuracy: 0.0156

Processing batch 87/125
Batch shape: torch.Size([128, 8])
Batch 87 - Loss: 46.3775, Accuracy: 0.0338

Processing batch 88/125
Batch shape: torch.Size([128, 8])
Batch 88 - Loss: 19.0883, Accuracy: 0.0210

Processing batch 89/125
Batch shape: torch.Size([128, 8])
Batch 89 - Loss: 24.6371, Accuracy: 0.0255

Processing batch 90/125
Batch shape: torch.Size([128, 8])
Batch 90 - Loss: 18.9036, Accuracy: 0.0210

Processing batch 91/125
Batch shape: torch.Size([128, 8])
Batch 91 - Loss: 17.4700, Accuracy: 0.0153

Processing batch 92/125
Batch shape: torch.Size([128, 8])
Batch 92 - Loss: 18.4704, Accuracy: 0.0164

Processing batch 93/125
Batch shape: torch.Size([128, 8])
Batch 93 - Loss: 24.9373, Accuracy: 0.0189

Processing batch 94/125
Batch shape: torch.Size([128, 8])
Batch 94 - Loss: 26.2023, Accuracy: 0.0581

Processing batch 95/125
Batch shape: torch.Size([128, 8])
Batch 95 - Loss: 14.6860, Accuracy: 0.1000

Processing batch 96/125
Batch shape: torch.Size([128, 8])
Batch 96 - Loss: 19.3658, Accuracy: 0.0210

Processing batch 97/125
Batch shape: torch.Size([128, 8])
Batch 97 - Loss: 18.8905, Accuracy: 0.0210

Processing batch 98/125
Batch shape: torch.Size([128, 8])
Batch 98 - Loss: 20.9945, Accuracy: 0.0192

Processing batch 99/125
Batch shape: torch.Size([128, 8])
Batch 99 - Loss: 37.0683, Accuracy: 0.0185

Processing batch 100/125
Batch shape: torch.Size([128, 8])
Batch 100 - Loss: 22.4727, Accuracy: 0.0151

Processing batch 101/125
Batch shape: torch.Size([128, 8])
Batch 101 - Loss: 24.1869, Accuracy: 0.0140

Processing batch 102/125
Batch shape: torch.Size([128, 8])
Batch 102 - Loss: 24.0608, Accuracy: 0.0153

Processing batch 103/125
Batch shape: torch.Size([128, 8])
Batch 103 - Loss: 39.4812, Accuracy: 0.0195

Processing batch 104/125
Batch shape: torch.Size([128, 8])
Batch 104 - Loss: 38.8453, Accuracy: 0.0217

Processing batch 105/125
Batch shape: torch.Size([128, 8])
Batch 105 - Loss: 39.1002, Accuracy: 0.0110

Processing batch 106/125
Batch shape: torch.Size([128, 8])
Batch 106 - Loss: 25.5789, Accuracy: 0.0157

Processing batch 107/125
Batch shape: torch.Size([128, 8])
Batch 107 - Loss: 32.0410, Accuracy: 0.0178

Processing batch 108/125
Batch shape: torch.Size([128, 8])
Batch 108 - Loss: 19.1877, Accuracy: 0.0227

Processing batch 109/125
Batch shape: torch.Size([128, 8])
Batch 109 - Loss: 35.2677, Accuracy: 0.0101

Processing batch 110/125
Batch shape: torch.Size([128, 8])
Batch 110 - Loss: 28.0705, Accuracy: 0.0189

Processing batch 111/125
Batch shape: torch.Size([128, 8])
Batch 111 - Loss: 32.0728, Accuracy: 0.0204

Processing batch 112/125
Batch shape: torch.Size([128, 8])
Batch 112 - Loss: 31.7721, Accuracy: 0.0532

Processing batch 113/125
Batch shape: torch.Size([128, 8])
Batch 113 - Loss: 17.7542, Accuracy: 0.0169

Processing batch 114/125
Batch shape: torch.Size([128, 8])
Batch 114 - Loss: 13.8716, Accuracy: 0.1136

Processing batch 115/125
Batch shape: torch.Size([128, 8])
Batch 115 - Loss: 49.5083, Accuracy: 0.0246

Processing batch 116/125
Batch shape: torch.Size([128, 8])
Batch 116 - Loss: 13.6769, Accuracy: 0.0753

Processing batch 117/125
Batch shape: torch.Size([128, 8])
Batch 117 - Loss: 13.0711, Accuracy: 0.0842

Processing batch 118/125
Batch shape: torch.Size([128, 8])
Batch 118 - Loss: 12.1983, Accuracy: 0.1800

Processing batch 119/125
Batch shape: torch.Size([128, 8])
Batch 119 - Loss: 21.7919, Accuracy: 0.0205

Processing batch 120/125
Batch shape: torch.Size([128, 8])
Batch 120 - Loss: 37.0991, Accuracy: 0.0182

Processing batch 121/125
Batch shape: torch.Size([128, 8])
Batch 121 - Loss: 24.2093, Accuracy: 0.0133

Processing batch 122/125
Batch shape: torch.Size([128, 8])
Batch 122 - Loss: 20.4193, Accuracy: 0.0179

Processing batch 123/125
Batch shape: torch.Size([128, 8])
Batch 123 - Loss: 41.2223, Accuracy: 0.0266

Processing batch 124/125
Batch shape: torch.Size([128, 8])
Batch 124 - Loss: 23.5062, Accuracy: 0.0101

Processing batch 125/125
Batch shape: torch.Size([128, 8])
Batch 125 - Loss: 30.6299, Accuracy: 0.0101

Evaluation complete!
Total tokens processed: 43263
Total correct predictions: 855

SuperReLoRA Model Metrics:
Loss: 25.8938
Perplexity: 176011948881.43
Accuracy: 0.0198

Generating example texts:

Prompt: Once upon a time
Generated: Once upon a time in which this is known , they are to become part of an armed and supporting the newly formed armored Army Corps for the battleship Princess Grace . The arrival at Prunella was unforgiven by Waffle , who was also the ongoing leader of the brigade , and at the end of the lengthy race whose plans were completely undecidable but not only as important for it . 
 Eight deaths after the German defences have been bound by the British capture of Portsmouth , England and the British Empire , and went without significant deadlines or discomfort during the Great Depression . 
 However , some of the British Armored Cavalry Regiment has been destroyed over all both the Royal Navy , including the Royal Navy village , that is the largest section which was destroyed . 
 At this point the first troops of the Royal Navy held their positions in the northern portions of the Southwark region . Both of these included the 9th Battalion ( the " B @-@ 10 " ) and 7th Division . 
 This was unknown from the 8th Brigade 's squadron . 
 As this unusual division detached the Royal Group on the frontier of the 2nd Brigade ; however , by 1850 it was possible that when the 1st Brigade had occupied around Heybridge , Northumberland , it had to be recovered nearby , but took up in mountainous terrain . 
 
 Along with two guns against the 1st Light Horse Brigade , a four @-@ facelock horn shirt commander was appointed as the officer , but his position was struck out and a few battalions wore in turn . It was subsequently broken up by tankers from the 3rd Regiment , whom the Royal Regiment was responsible for damage and evacuation . 
 Upon returning from Inverness and eventually abandoning North Yorkshire , Owen sent back to the 5th Company to create units of infantry and artillery forces , which would provide for more than 1 @,@ 000 casualties into the line . With the advance of the 1st Infantry Brigade , Onion had soldiers and infantry troopshipmen to the 6th Branch Division , but the left @-@ handed French engineers did not attack the infantry rope based on the town of Ortona , but would not go on to defend the regiment . 
 
 In addition to this time , the 22nd Division commanded several artillery four officers . 
 Notchfulness from the Battle of Heligoland was later recalled , soon afterwards , he was saved from the 4th Engineer Brigade on 14 February and began searching for the battleline for ten days . 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Race of the Day 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


Prompt: The most important thing about
Generated: The most important thing about the war in Somalia was to be the effects of Somali 's independence . For instance , it is that Somaliland and Somali government should have a majority power on their independence . The Somali @-@ belt government also supported the immediate decision in which they could establish throughout the country , even uneven independently for the nation . 
 Reactions from Somali movement made by Nuskhuri in the Biddenden Maids are noted as " the greatest and more popular achievement for the Somali and Somali language and humanities ; but sometimes there will not occasionally affect this decision , and there will be so five years including India . [ ... ] Anonymous has a few hundred people who have strong information about the Somali and Somali languages and the development of a common religious difference . That is not until the late 1980s , but there are other partners such as the Somali @-@ belt governments with their oppositions . " 
 These changes have begun to use some specialist aspects : a change in form of local authorities involved recognition of somalised independencies between the nations , and a security situation in what is known for its environmental and social control . 
 
 
 Notas 
 In the 1970s she was given a way to help the city return to Mogadishu toward the city . 
 While working over Somalia and the Somali written settlements , Sassanism and Nazi Muslims left Mogadishu into the city . 
 A day after the Somali authoritarian Department of Transport itself began to create and implement modern facilities . 
 It is possible that Somali and Somali were all under the loss of whatever the city has saved during the battles of Kismayanoba and Kanak and where somali was inducted the country 's military status . 
 
 According to an online article published in the Philippines News , she recalls that Somali government inherited the city due to the military status . Officials and foreign governors criticized her for captivity , while citing several plans . 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


Prompt: In the future, artificial intelligence will
Generated: In the future, artificial intelligence will be used to create a little interest in the process of cognitivity . However , instead of symbolization is needed by complex thoughts with relatives of logic and an alternative change between logical concept programming ( ld1 ) and functional analysis ( LDA ) . Similar differences have developed in both cells such as whereas modeling biases are more important than algorithms or changes which cannot be shown from possible to mankind that effects may actually be connected to solvations made on other computers and for example a number of first @-@ class computation computors are closely related to " automated algebra " . 
 A study suggested that identity recognition needs needed for a state — without reliability – when a " linear algorithm " mechanic was used , based on probabilistic variables . 
 The proportion of human health and high quality heuristics has increased encounterly , although some individuals continues to use this type of function . 
 Construction problem for determinants in hypotheses is to investigate the results of a generalized resolution or model of linear algebra ; however , the solved specific problems can be called in a " unique system " overviewed into all cases . 
 Solve the problem of determinating whether the result of a good schematic or algorithm matched used as partially discredits within the context of the series . 
 For example , a single graph forming the solving and decisive problem for a topological fraction is could leave . 
 This problem arises about the length of solver algorithms and of constructing the solverall arrangement . 
 Finding the solvable solver algorithm matching lower than beginning with an optimism caused by the solvers proposal ; such solves low themselves that the solverall arrangement are still discredited at their respective limitations . 
 A common problem of determining the solvability of a maximum size is the determination of the solution in general activity . 
 For example , it is disorganized due to the solving of what she belongs to him . 
 Reflections occurring in binary solve principles which make such solutions such as the reliability of each solverage to prove " one hand is necessary " and the determinance of " one hand is possible " . 
 Delayed problems that no graphs each of these solvae are likeless ; if certainty or informational problems include the solve the solving of what it seems to have doublished solvings of the solving of " one of them ... the solution aimed to discuss solutions into solvents " . 
 
 Some solutions are not unclear . 
 One solution is to prevent the solving of those solved solving solvements . 
 Two solutions are plausible : 
 
 As the solution is to meet solutions if problems are perceived directly after solving a solution . 
 When solving solutions are the most full or fully satisfactory , the solution is quite simplistic . 
 Three solutions are likelihoods . 
 Other solution feels thinking or solving of the solution has been mistakenly present during a solution by quantity . 
 Delayed problems are two solutions that are like a " combination " of solutions that are pointed out of any known plan called solutions . 
 Some solutions are uncompleted . 
 Two lower solutions are unable to find solutions often understand how she is very much more than the solution in all cases . 
 Seeing solutions have been produced in space within the problem for
